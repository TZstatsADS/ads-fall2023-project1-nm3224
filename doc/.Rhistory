# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx_m <- slam::row_sums(DTM) > 0
DTM_male <- DTM_male[sel_idx_m, ]
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel_m <- LDA(DTM_m, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# Topics for Males
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM_m <- DocumentTermMatrix(m_2$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM_m)
# number of topics
K <- 14
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx_m <- slam::row_sums(DTM) > 0
DTM_m <- DTM_m[sel_idx_m, ]
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel_m <- LDA(DTM_m, K, method="Gibbs", control=list(iter = 500, verbose = 25))
rm(DTM_male)
# Topics for Males
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM_m <- DocumentTermMatrix(m_2$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM_m)
# number of topics
K <- 14
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx_m <- slam::row_sums(DTM) > 0
DTM_m <- DTM_m[sel_idx_m, ]
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel_m <- LDA(DTM_m, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# Topics for Males
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM_m <- DocumentTermMatrix(m_2$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM_m)
# number of topics
K <- 14
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx_m <- slam::row_sums(DTM) > 0
DTM_m <- DTM_m[sel_idx_m, ]
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel_m <- LDA(DTM_m, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# Topics for Females
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM_f <- DocumentTermMatrix(f_1$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM_f)
# number of topics
# I had to do some parameter tuning to find one that reflected the topic "romance"
K <- 14
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx_f <- slam::row_sums(DTM_f) > 0
DTM_f <- DTM_f[sel_idx_f, ]
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel_f <- LDA(DTM_f, K, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel_f, 10)
# Topics for Males
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM_m <- DocumentTermMatrix(m_2$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM_m)
# number of topics
K <- 14
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx_m <- slam::row_sums(DTM) > 0
DTM_m <- DTM_m[sel_idx_m, ]
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel_m <- LDA(DTM_m, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# Topics for Males
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM_m <- DocumentTermMatrix(m_2$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM_m)
# number of topics
K <- 14
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx_m <- slam::row_sums(DTM_m) > 0
DTM_m <- DTM_m[sel_idx_m, ]
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel_m <- LDA(DTM_m, K, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel_m, 10)
View(f_1)
```{r}
love <- c('wife', 'husband', 'kiss', 'date', 'boyfriend', 'girlfriend', 'fiance', 'fiancee', 'engaged', 'sex', 'sexual', 'dating', 'romance', 'romantic', 'spouse', 'partner', 'lover')
romance <- subset(d, word %in% love)
#for some reason this code doesn't work unless I execute the steps in this stackoverflow link: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
happy_text <- subset(cleaned_data, text %in% love)
View(happy_text)
happy_text <- subset(cleaned_data, text %in% c('happy'))
View(happy_text)
happy_text <- subset(cleaned_data, text %in% 'happy')
happy_text <- subset(cleaned_data, grepl('happy', cleaned_data$text))
View(happy_text)
happy_text <- subset(cleaned_data, grepl('happy', cleaned_data$text))
#  Overall topics
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(happy_text$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
# number of topics
# I had to do some parameter tuning to find one that reflected the topic "romance"
K <- 14
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel, 10)
View(happy_text)
happy_text <- subset(cleaned_data, grepl('partner', cleaned_data$text))
#  Overall topics
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(happy_text$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
# number of topics
# I had to do some parameter tuning to find one that reflected the topic "romance"
K <- 14
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel, 10)
View(happy_text)
love <- c('wife', 'husband', 'kiss', 'date', 'boyfriend', 'girlfriend', 'fiance', 'fiancee', 'engaged', 'sex', 'sexual', 'dating', 'romance', 'romantic', 'spouse', 'partner', 'lover', 'marriage')
#Overall frequency of romance across both categories
romance <- subset(d, word %in% love)
#Subsetting by gender
romance_male <- subset(d_m, word %in% love)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# visualize topics as word cloud
topicToViz <- 11 # change for your own topic of interest
topicToViz <- grep('husband', topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
View(tmResult)
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
#for sentiment analysis
library(syuzhet)
installed.packages('syuzhet')
installed.packages('lubridate')
installed.packages('scales')
installed.packages('reshape2')
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
#for sentiment analysis
library(syuzhet)
installed.packages('syuzhet')
install.packages('syuzhet')
install.packages('lubridate')
install.packages("lubridate")
install.packages('scales')
install.packages('reshape2')
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
#for sentiment analysis
library(syuzhet)
library(lubridate)
library(scales)
library(reshape2)
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('../data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('../data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
## Per gender
gender_df <- merge(cleaned_data, demo_data)
#for some reason this code doesn't work unless I execute the steps in this stackoverflow link: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#FEMALE vs MALE
f_1 <- subset(gender_df, gender_df$gender == 'f')
m_2 <- subset(gender_df, gender_df$gender == 'm')
love <- c('wife', 'husband', 'kiss', 'date', 'boyfriend', 'girlfriend', 'fiance', 'fiancee', 'engaged', 'sex', 'sexual', 'dating', 'romance', 'romantic', 'spouse', 'partner', 'lover', 'marriage')
#Overall frequency of romance across both categories
romance <- subset(d, word %in% love)
#Subsetting by gender
romance_male <- subset(d_m, word %in% love)
#Most frequent terms for males discussing happy moments:
dtm_m <- TermDocumentMatrix(m_2$text)
m_m <- as.matrix(dtm_m)
v_m <- sort(rowSums(m_m),decreasing=TRUE)
d_m <- data.frame(word = names(v_m),freq=v_m)
head(d_m, 10)
#Most frequent terms for females discussing happy moments:
dtm_f <- TermDocumentMatrix(f_1$text)
m_f <- as.matrix(dtm_f)
v_f <- sort(rowSums(m_f),decreasing=TRUE)
d_f <- data.frame(word = names(v_f),freq=v_f)
head(d_f, 10)
#Overall frequency of romance across both categories
romance <- subset(d, word %in% love)
#Subsetting by gender
romance_male <- subset(d_m, word %in% love)
romance_male$Gender <- 'Male'
romance_female <- subset(d_f, word %in% love)
romance_female$Gender <- 'Female'
#Compute average frequency for word for each gender category by dividing frequency for gender category by overall frequency over all documents
romance_male$avg_freq <- romance_male$freq / romance$freq
romance_female$avg_freq <- romance_female$freq / romance$freq
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity") +
theme(axis.text.x = element_text(angle = 90))
ggsave(filename = file.path("../figs", 'romantic_words_plot.png'))
partner_text <- subset(cleaned_data, text %in% love)
View(partner_text)
partner_text <- subset(cleaned_data, grepl(cleaned_data$text %in% love))
partner_text <- gender_df[grep(paste(love, collapse='|'), gender_df$text, ignore.case=TRUE),]
View(partner_text)
partner <- iconv(partner_text$text)
partner_text <- gender_df[grep(paste(love, collapse='|'), gender_df$text, ignore.case=TRUE),]
partner <- iconv(partner_text$text)
s <- get_nrc_sentiment(partner)
head(s)
getwd()
setwd('Desktop/GitHub/ads-fall2023-project1-nm3224/doc/')
partner_text <- gender_df[grep(paste(love, collapse='|'), gender_df$text, ignore.case=TRUE),]
partner_mom <- iconv(partner_text$text)
s <- get_nrc_sentiment(partner_mom)
View(s)
head(s)
get_nrc_sentiment(partner)
get_nrc_sentiment(partner)
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores Tweets')
View(m_2)
partner_text_male <- m_2[grep(paste(love, collapse='|'), m_2$text, ignore.case=TRUE),]
partner_male <- iconv(partner_text_male$text)
s_male <- get_nrc_sentiment(partner_male)
partner_text_female <- f_1[grep(paste(love, collapse='|'), f_1$text, ignore.case=TRUE),]
partner_female <- iconv(partner_text_female$text)
s_female <- get_nrc_sentiment(partner_female)
View(s_male)
View(s_male)
colSums(s_male)
View(romance_df)
sent_m <- colSums(s_male)
df_m <- data.frame(sent_m)
View(df_m)
sent_m <- colSums(s_male)
df_m <- data.frame(sent_m, c('Male'))
View(df_m)
df_m <- data.frame(sent_m, 'Male')
View(df_m)
df_m <- data.frame(sent_m, Gender = c('Male'))
View(df_m)
sent_m <- colSums(s_male)
df_m <- data.frame(sent_m, Gender = c('Male'))
df_m <- cbind(newColName = rownames(df_m), df_m)
View(df_m)
df_m <- cbind(feeling = rownames(df_m), df_m)
View(df_m)
rm(df_m)
sent_m <- colSums(s_male)
df_m <- data.frame(sent_m, Gender = c('Male'))
df_m <- cbind(feeling = rownames(df_m), df_m)
sent_f <- colSums(s_female)
df_f <- data.frame(sent_f, Gender = c('Female'))
df_f <- cbind(feeling = rownames(df_f), df_f)
sents <- rbing(df_m, df_f)
sents <- rbind(df_m, df_f)
View(d_f)
View(df_f)
View(df_m)
rm(df_m)
rm(df_f)
sent <- colSums(s_male)
df_m <- data.frame(sent_m, Gender = c('Male'))
df_m <- cbind(feeling = rownames(df_m), df_m)
sent <- colSums(s_female)
df_f <- data.frame(sent_f, Gender = c('Female'))
df_f <- cbind(feeling = rownames(df_f), df_f)
sents <- rbind(df_m, df_f)
View(df_f)
View(df_m)
rm(df_f)
rm(df_m)
sent <- colSums(s_male)
df_m <- data.frame(sent, Gender = c('Male'))
df_m <- cbind(feeling = rownames(df_m), df_m)
sent <- colSums(s_female)
df_f <- data.frame(sent, Gender = c('Female'))
df_f <- cbind(feeling = rownames(df_f), df_f)
sents <- rbind(df_m, df_f)
View(sents)
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores for Partner-Related Tweets')
png("../figs/barplot_all.png")
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores for Partner-Related Tweets')
dev.off()
View(sents)
ggplot(sents, aes(fill=Gender, y=sent, x=feeling)) +
geom_bar(position="dodge", stat="identity") +
theme(axis.text.x = element_text(angle = 90))
ggsave(filename = file.path("../figs", 'sentiment_plot_genders.png'))
# install.packages('package') for each package listed.
# setwd('Desktop/GitHub/ads-fall2023-project1-nm3224/doc/')
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
#for sentiment analysis
library(syuzhet)
library(lubridate)
library(scales)
library(reshape2)
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('../data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
## Per gender
gender_df <- merge(cleaned_data, demo_data)
#for some reason this code doesn't work unless I execute the steps in this stackoverflow link: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#FEMALE vs MALE
f_1 <- subset(gender_df, gender_df$gender == 'f')
m_2 <- subset(gender_df, gender_df$gender == 'm')
#Most frequent terms for females discussing happy moments:
dtm_f <- TermDocumentMatrix(f_1$text)
m_f <- as.matrix(dtm_f)
v_f <- sort(rowSums(m_f),decreasing=TRUE)
d_f <- data.frame(word = names(v_f),freq=v_f)
head(d_f, 10)
#Most frequent terms for males discussing happy moments:
dtm_m <- TermDocumentMatrix(m_2$text)
m_m <- as.matrix(dtm_m)
v_m <- sort(rowSums(m_m),decreasing=TRUE)
d_m <- data.frame(word = names(v_m),freq=v_m)
head(d_m, 10)
love <- c('wife', 'husband', 'kiss', 'date', 'boyfriend', 'girlfriend', 'fiance', 'fiancee', 'engaged', 'sex', 'sexual', 'dating', 'romance', 'romantic', 'spouse', 'partner', 'lover', 'marriage')
#Overall frequency of romance across both categories
romance <- subset(d, word %in% love)
#Subsetting by gender
romance_male <- subset(d_m, word %in% love)
romance_male$Gender <- 'Male'
romance_female <- subset(d_f, word %in% love)
romance_female$Gender <- 'Female'
#Compute average frequency for word for each gender category by dividing frequency for gender category by overall frequency over all documents
romance_male$avg_freq <- romance_male$freq / romance$freq
romance_female$avg_freq <- romance_female$freq / romance$freq
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity") +
theme(axis.text.x = element_text(angle = 90))
ggsave(filename = file.path("../figs", 'romantic_words_plot.png'))
## Overall vocabulary between happiness and animals
x <- tolower(cleaned_data$cleaned_hm)
cat(x[1])
process_data <- function(x, n_topics){
anno <- udpipe(x, "english", trace = 10, parallel.cores = 1)
anno <- subset(anno, !is.na(lemma) & nchar(lemma) > 1 & !upos %in% "PUNCT")
anno$text <- sprintf("%s//%s", anno$lemma, anno$upos)
x <- paste.data.frame(anno, term = "text", group = "doc_id", collapse = " ")
model <- word2vec(x = x$text, dim = n_topics, iter = 20, split = c(" ", ".\n?!"))
embedding <- as.matrix(model)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding)
df <- data.frame(word = gsub("//.+", "", rownames(viz)),
upos = gsub(".+//", "", rownames(viz)),
x = viz[, 1], y = viz[, 2],
stringsAsFactors = FALSE)
df <- subset(df, upos %in% c("ADJ", "NOUN"))
return(list('data'=df,'model'=model))
}
l <- process_data(x, 15)
lookslike2 <- predict(l$model, c("partner//NOUN"), type = "nearest", top_n = 10)
word_list_partner <- lookslike2$`partner//NOUN`$term2
word_list_happiness <- predict(l$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
partner_df <- subset(l$data, rownames(l$data) %in% word_list_partner)
happiness_df <- subset(l$data, rownames(l$data) %in% word_list_happiness)
options(ggrepel.max.overlaps = Inf)
ggplot(partner_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=partner_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to partner with word2vec - umap")
ggsave(filename = file.path("../figs", 'word2vec_overall.png'))
x_1 <- tolower(f_1$cleaned_hm)
x_2 <- tolower(m_2$cleaned_hm)
l_1 <- process_data(x_1, 15)
l_2 <- process_data(x_2, 15)
word_list_partner1 <- predict(l_1$model, c("partner//NOUN"), type = "nearest", top_n = 10)$`partner//NOUN`$term2
word_list_happiness1 <- predict(l_1$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
word_list_partner2 <- predict(l_2$model, c("partner//NOUN"), type = "nearest", top_n = 10)$`partner//NOUN`$term2
word_list_happiness2 <- predict(l_2$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
partner1 <- subset(l_1$data, rownames(l_1$data) %in% word_list_partner1)
happiness1 <- subset(l_1$data, rownames(l_1$data) %in% word_list_happiness1)
partner2 <- subset(l_2$data, rownames(l_2$data) %in% word_list_partner2)
happiness2 <- subset(l_2$data, rownames(l_2$data) %in% word_list_happiness2)
options(ggrepel.max.overlaps = Inf)
ggplot(partner1, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness1, aes(x=x, y=y, label=word, color='red')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness2, aes(x=x, y=y, label=word, color='magenta')) +
geom_text_repel() + theme_void() + geom_text_repel(data=partner2, aes(x=x, y=y, label=word, color='cyan')) + scale_color_manual(values=c("blue", "red", "magenta", 'cyan'),
labels = c("women - partner", "men - partner", "men - happiness", 'women - happiness'))
labs(title = "Most similar words to partner with word2vec - umap")
ggsave('../figs/word2vec_genres.png')
partner_text_male <- gender_df[grep(paste(love, collapse='|'), gender_df$text, ignore.case=TRUE),]
partner_mom <- iconv(partner_text$text)
