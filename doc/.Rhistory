library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(plyr)
library(tm)
library(wordcloud)
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('https://raw.githubusercontent.com/megagonlabs/HappyDB/master/happydb/data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
#make all text lowercase
cleaned_data <- cleaned_data %>%
mutate(cleaned_hm = tolower(cleaned_hm))
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('../data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
#make all text lowercase
cleaned_data <- cleaned_data %>%
mutate(cleaned_hm = tolower(cleaned_hm))
## Per gender
gender_df <- merge(cleaned_data, demo_data)
#for some reason this code doesn't work unless I execute the steps in this stackoverflow link: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#FEMALE vs MALE
f_1 <- subset(gender_df, gender_df$gender == 'f')
m_2 <- subset(gender_df, gender_df$gender == 'm')
#Most frequent terms for females discussing happy moments:
dtm_f <- TermDocumentMatrix(f_1$text)
m_f <- as.matrix(dtm_f)
v_f <- sort(rowSums(m_f),decreasing=TRUE)
d_f <- data.frame(word = names(v_f),freq=v_f)
head(d_f, 10)
#Most frequent terms for males discussing happy moments:
dtm_m <- TermDocumentMatrix(m_2$text)
m_m <- as.matrix(dtm_m)
v_m <- sort(rowSums(m_m),decreasing=TRUE)
d_m <- data.frame(word = names(v_m),freq=v_m)
head(d_m, 10)
love <- c('wife', 'husband', 'kiss', 'date', 'boyfriend', 'girlfriend', 'fiance', 'fiancee', 'engaged', 'sex', 'sexual', 'dating', 'romance', 'romantic', 'spouse', 'partner', 'lover')
#Overall frequency of romance across both categories
romance <- subset(d, word %in% love)
#Subsetting by gender
romance_male <- subset(d_m, word %in% love)
romance_male$Gender <- 'Male'
romance_female <- subset(d_f, word %in% love)
romance_female$Gender <- 'Female'
#Compute average frequency for word for each gender category by dividing frequency for gender category by overall frequency over all documents
romance_male$avg_freq <- romance_male$freq / romance$freq
romance_female$avg_freq <- romance_female$freq / romance$freq
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity")
save.image('../figs/romantic_words_plot.png')
pdf(file = '../figs/romantic_words_plot.png',
width = 7,
height = 5)
dev.off()
png(file = '../figs/romantic_words_plot.png',
width = 7,
height = 5)
dev.off()
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity")
png(file = '../figs/romantic_words_plot.png',
width = 7,
height = 5)
dev.off()
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity")
png(file = '../figs/romantic_words_plot.png',
width = 7,
height = 5)
dev.off()
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity")
ggsave(filename = file.path("'../figs", 'romantic_words_plot.png'))
getwd()
setwd('ads-fall2023-project1-nm3224/doc')
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity")
ggsave(filename = file.path("'../figs", 'romantic_words_plot.png'))
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity")
ggsave(filename = file.path("../figs", 'romantic_words_plot.png'))
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity") +
theme(axis.text.x = element_text(angle = 90))
ggsave(filename = file.path("../figs", 'romantic_words_plot.png'))
## Overall vocabulary between happiness and animals
x <- tolower(cleaned_data$cleaned_hm)
cat(x[1])
process_data <- function(x, n_topics){
anno <- udpipe(x, "english", trace = 10, parallel.cores = 1)
anno <- subset(anno, !is.na(lemma) & nchar(lemma) > 1 & !upos %in% "PUNCT")
anno$text <- sprintf("%s//%s", anno$lemma, anno$upos)
x <- paste.data.frame(anno, term = "text", group = "doc_id", collapse = " ")
model <- word2vec(x = x$text, dim = n_topics, iter = 20, split = c(" ", ".\n?!"))
embedding <- as.matrix(model)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding)
df <- data.frame(word = gsub("//.+", "", rownames(viz)),
upos = gsub(".+//", "", rownames(viz)),
x = viz[, 1], y = viz[, 2],
stringsAsFactors = FALSE)
df <- subset(df, upos %in% c("ADJ", "NOUN"))
return(list('data'=df,'model'=model))
}
l <- process_data(x, 15)
library(word2vec)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
process_data <- function(x, n_topics){
anno <- udpipe(x, "english", trace = 10, parallel.cores = 1)
anno <- subset(anno, !is.na(lemma) & nchar(lemma) > 1 & !upos %in% "PUNCT")
anno$text <- sprintf("%s//%s", anno$lemma, anno$upos)
x <- paste.data.frame(anno, term = "text", group = "doc_id", collapse = " ")
model <- word2vec(x = x$text, dim = n_topics, iter = 20, split = c(" ", ".\n?!"))
embedding <- as.matrix(model)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding)
df <- data.frame(word = gsub("//.+", "", rownames(viz)),
upos = gsub(".+//", "", rownames(viz)),
x = viz[, 1], y = viz[, 2],
stringsAsFactors = FALSE)
df <- subset(df, upos %in% c("ADJ", "NOUN"))
return(list('data'=df,'model'=model))
}
l <- process_data(x, 15)
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
process_data <- function(x, n_topics){
anno <- udpipe(x, "english", trace = 10, parallel.cores = 1)
anno <- subset(anno, !is.na(lemma) & nchar(lemma) > 1 & !upos %in% "PUNCT")
anno$text <- sprintf("%s//%s", anno$lemma, anno$upos)
x <- paste.data.frame(anno, term = "text", group = "doc_id", collapse = " ")
model <- word2vec(x = x$text, dim = n_topics, iter = 20, split = c(" ", ".\n?!"))
embedding <- as.matrix(model)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding)
df <- data.frame(word = gsub("//.+", "", rownames(viz)),
upos = gsub(".+//", "", rownames(viz)),
x = viz[, 1], y = viz[, 2],
stringsAsFactors = FALSE)
df <- subset(df, upos %in% c("ADJ", "NOUN"))
return(list('data'=df,'model'=model))
}
l <- process_data(x, 15)
lookslike2 <- predict(l$model, c("partner//NOUN"), type = "nearest", top_n = 10)
word_list_partner <- lookslike2$`animal//NOUN`$term2
word_list_happiness <- predict(l$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
partner_df <- subset(l$data, rownames(l$data) %in% word_list_partner)
happiness_df <- subset(l$data, rownames(l$data) %in% word_list_happiness)
options(ggrepel.max.overlaps = Inf)
ggplot(partner_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to partner with word2vec - umap")
ggsave(filename = file.path("../figs", 'word2vec_overall.png'))
x_1 <- tolower(f_1$cleaned_hm)
x_2 <- tolower(m_2$cleaned_hm)
l_1 <- process_data(x_1, 15)
options(ggrepel.max.overlaps = Inf)
ggplot(partner_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=partner_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to partner with word2vec - umap")
ggsave(filename = file.path("../figs", 'word2vec_overall.png'))
options(ggrepel.max.overlaps = Inf)
ggplot(partner_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=partner_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to partner with word2vec - umap")
ggsave(filename = file.path("../figs", 'word2vec_overall.png'))
View(partner_df)
partner_df <- subset(l$data, rownames(l$data) %in% word_list_partner)
happiness_df <- subset(l$data, rownames(l$data) %in% word_list_happiness)
lookslike2 <- predict(l$model, c("partner//NOUN"), type = "nearest", top_n = 10)
word_list_partner <- lookslike2$`partner//NOUN`$term2
word_list_happiness <- predict(l$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
partner_df <- subset(l$data, rownames(l$data) %in% word_list_partner)
happiness_df <- subset(l$data, rownames(l$data) %in% word_list_happiness)
lookslike2 <- predict(l$model, c("partner//NOUN"), type = "nearest", top_n = 10)
word_list_partner <- lookslike2$`partner//NOUN`$term2
word_list_happiness <- predict(l$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
View(partner_df)
partner_df <- subset(l$data, rownames(l$data) %in% word_list_partner)
happiness_df <- subset(l$data, rownames(l$data) %in% word_list_happiness)
options(ggrepel.max.overlaps = Inf)
ggplot(partner_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=partner_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to partner with word2vec - umap")
ggsave(filename = file.path("../figs", 'word2vec_overall.png'))
x_1 <- tolower(f_1$cleaned_hm)
x_2 <- tolower(m_2$cleaned_hm)
l_1 <- process_data(x_1, 15)
l_2 <- process_data(x_2, 15)
word_list_partner1 <- predict(l_1$model, c("partner//NOUN"), type = "nearest", top_n = 10)$`partner//NOUN`$term2
word_list_happiness1 <- predict(l_1$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
word_list_partner2 <- predict(l_2$model, c("partner//NOUN"), type = "nearest", top_n = 10)$`partner//NOUN`$term2
word_list_happiness2 <- predict(l_2$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
partner1 <- subset(l_1$data, rownames(l_1$data) %in% word_list_partner1)
happiness1 <- subset(l_1$data, rownames(l_1$data) %in% word_list_happiness1)
partner2 <- subset(l_2$data, rownames(l_2$data) %in% word_list_partner2)
happiness2 <- subset(l_2$data, rownames(l_2$data) %in% word_list_happiness2)
options(ggrepel.max.overlaps = Inf)
ggplot(partner1, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness1, aes(x=x, y=y, label=word, color='red')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness2, aes(x=x, y=y, label=word, color='magenta')) +
geom_text_repel() + theme_void() + geom_text_repel(data=animal2, aes(x=x, y=y, label=word, color='cyan')) + scale_color_manual(values=c("blue", "red", "magenta", 'cyan'),
labels = c("women - partner", "men - partner", "men - happiness", 'women - happiness'))
options(ggrepel.max.overlaps = Inf)
ggplot(partner1, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness1, aes(x=x, y=y, label=word, color='red')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness2, aes(x=x, y=y, label=word, color='magenta')) +
geom_text_repel() + theme_void() + geom_text_repel(data=partner2, aes(x=x, y=y, label=word, color='cyan')) + scale_color_manual(values=c("blue", "red", "magenta", 'cyan'),
labels = c("women - partner", "men - partner", "men - happiness", 'women - happiness'))
labs(title = "Most similar words to partner with word2vec - umap")
ggsave('../figs/word2vec_genres.png')
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
#for topic modeling
library(topicmodels)
install.packages("topicmodels")
install.packages("reshape2")
install.packages("pals")
install.packages("SnowballC")
install.packages("SnowballC")
install.packages("lda")
install.packages("ldatuning")
install.packages("kableExtra")
install.packages("DT")
install.packages("flextable")
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
#for topic modeling
library(topicmodels)
library(reshape2)
library(pals)
library(SnowballC)
library(lda)
library(ldatuning)
library(kableExtra)
library(DT)
library(flextable)
# number of topics
K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('../data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(cleaned_hm$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(cleaned_data$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
# number of topics
K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel, 10)
## Per gender
gender_df <- merge(cleaned_data, demo_data)
#FEMALE vs MALE
f_1 <- subset(gender_df, gender_df$gender == 'f')
m_2 <- subset(gender_df, gender_df$gender == 'm')
# Topics for Females
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(f_1$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
# number of topics
K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel, 10)
# Topics for Males
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(m_2$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
# number of topics
K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel, 10)
# Topics for Males
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(m_2$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
# number of topics
K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel, 10)
# Topics for Males
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(m_2$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
# number of topics
K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel, 10)
# visualize topics as word cloud
topicToViz <- 11 # change for your own topic of interest
topicToViz <- grep('husband', topicNames)[1] # Or select a topic by a term contained in its name
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
# visualize topics as word cloud
topicToViz <- 11 # change for your own topic of interest
topicToViz <- grep('husband', topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# visualize topics as word cloud
topicToViz <- 11 # change for your own topic of interest
topicToViz <- grep('husband', topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
View(tmResult)
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
beta <- tmResult$terms
dim(beta)
theta <- tmResult$topics
dim(theta)
# visualize topics as word cloud
topicToViz <- 11 # change for your own topic of interest
topicToViz <- grep('husband', topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
