---
title: "Project 1"
author: "Noreen Mayat"
output: html_document
---

*Question*: How do romantic relationships, experiences and interactions impact happiness across gender groups? 

In this project, I explore how romantic relationships, experiences and interactions impact happiness across gender groups of male and female, through computing the average word frequency for a bag of "romance"-related words for male and female, LDA topic modeling, and word2vec. 

Make sure to install all packages below before running any code; I installed them by running: 
```{r}
#install.packages('package') for each package listed.
```

```{r load_libraries, warning=FALSE, echo=FALSE}
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)

#for topic modeling 
library(topicmodels)
library(reshape2)
library(pals)
library(SnowballC)
library(lda)
library(ldatuning)
library(kableExtra)
library(DT)
library(flextable)
```

```{r data, warning=FALSE, echo=FALSE}
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('../data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
```

We need to combine `demo_data$gender` to `cleaned_data`, join on `wid` to compare gender groups.

```{r process data, echo=FALSE}
## Per gender 
gender_df <- merge(cleaned_data, demo_data)
```

Let's count the frequency of all words in our documents first.

```{r}
#for some reason this code doesn't work unless I execute the steps in this stackoverflow link: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached

dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```

Now, let's compare the frequency of words, stratified by each gender group.
Note that we have roughly the same number of observations for each group, with slightly more testimonies from males. There are some 42,019 observations labeled female, and 57,597 observations labeled female. 
```{r}
#FEMALE vs MALE
f_1 <- subset(gender_df, gender_df$gender == 'f')
m_2 <- subset(gender_df, gender_df$gender == 'm')
```

```{r}
#Most frequent terms for females discussing happy moments:

dtm_f <- TermDocumentMatrix(f_1$text)
m_f <- as.matrix(dtm_f)
v_f <- sort(rowSums(m_f),decreasing=TRUE)
d_f <- data.frame(word = names(v_f),freq=v_f)
head(d_f, 10)
```

```{r}
#Most frequent terms for males discussing happy moments:

dtm_m <- TermDocumentMatrix(m_2$text)
m_m <- as.matrix(dtm_m)
v_m <- sort(rowSums(m_m),decreasing=TRUE)
d_m <- data.frame(word = names(v_m),freq=v_m)
head(d_m, 10)
```

I don't see any major discrepancies off the bat; it may be more useful to now zone in on a specific bag of words I came up with related to romance, relationships, intimacy, and partnership.
```{r}
love <- c('wife', 'husband', 'kiss', 'date', 'boyfriend', 'girlfriend', 'fiance', 'fiancee', 'engaged', 'sex', 'sexual', 'dating', 'romance', 'romantic', 'spouse', 'partner', 'lover')
```
 
```{r}
#Overall frequency of romance across both categories
romance <- subset(d, word %in% love)

#Subsetting by gender
romance_male <- subset(d_m, word %in% love)
romance_male$Gender <- 'Male'

romance_female <- subset(d_f, word %in% love)
romance_female$Gender <- 'Female'

#Compute average frequency for word for each gender category by dividing frequency for gender category by overall frequency over all documents
romance_male$avg_freq <- romance_male$freq / romance$freq
romance_female$avg_freq <- romance_female$freq / romance$freq
```

```{r}
romance_df <- rbind(romance_male, romance_female) 

ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) + 
  geom_bar(position="dodge", stat="identity") +
  theme(axis.text.x = element_text(angle = 90))

ggsave(filename = file.path("../figs", 'romantic_words_plot.png'))
```

It seems men that the average frequency for romance-related words is higher in the male category than the female category. This means men discuss their romantic lives and relationships when talking about their happy moments more than women. This could imply that romantic relationships, interactions, and experiences are more impactful and related to male happiness than female happiness. 


Now, using word2vec: first, we convert the data to a list of characters to input into our model. 
```{r conversion}
## Overall vocabulary between happiness and animals
x <- tolower(cleaned_data$cleaned_hm)
cat(x[1])
```

Lemmatizing our text and using speech tag (verb, adverb, noun, adjective) will make representation easier (let's say we want to see all adjectives and nouns relative to the topic of animals).
```{r speech tagging and lemmatizing, echo=FALSE}
process_data <- function(x, n_topics){
  anno <- udpipe(x, "english", trace = 10, parallel.cores = 1)
  anno <- subset(anno, !is.na(lemma) & nchar(lemma) > 1 & !upos %in% "PUNCT")
  anno$text <- sprintf("%s//%s", anno$lemma, anno$upos)
  x <- paste.data.frame(anno, term = "text", group = "doc_id", collapse = " ")
  model <- word2vec(x = x$text, dim = n_topics, iter = 20, split = c(" ", ".\n?!"))
  embedding <- as.matrix(model)

  viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
  rownames(viz) <- rownames(embedding)

  df <- data.frame(word = gsub("//.+", "", rownames(viz)),
  upos = gsub(".+//", "", rownames(viz)),
  x = viz[, 1], y = viz[, 2],
  stringsAsFactors = FALSE)
  df <- subset(df, upos %in% c("ADJ", "NOUN"))
  return(list('data'=df,'model'=model))
}

l <- process_data(x, 15)
```

We now want to get the words most similar to partner in the embedding and we compare them to the words most similar to happy to observe a relationship. 
```{r most similar to partner, echo=FALSE}
lookslike2 <- predict(l$model, c("partner//NOUN"), type = "nearest", top_n = 10)
word_list_partner <- lookslike2$`partner//NOUN`$term2
word_list_happiness <- predict(l$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
```

```{r slicing, echo=FALSE}
partner_df <- subset(l$data, rownames(l$data) %in% word_list_partner)
happiness_df <- subset(l$data, rownames(l$data) %in% word_list_happiness)
```


```{r umap plot, echo=FALSE}
options(ggrepel.max.overlaps = Inf) 
ggplot(partner_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=partner_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to partner with word2vec - umap")
ggsave(filename = file.path("../figs", 'word2vec_overall.png'))
```

## Per gender 
```{r}
x_1 <- tolower(f_1$cleaned_hm)
x_2 <- tolower(m_2$cleaned_hm)

l_1 <- process_data(x_1, 15)
l_2 <- process_data(x_2, 15)

word_list_partner1 <- predict(l_1$model, c("partner//NOUN"), type = "nearest", top_n = 10)$`partner//NOUN`$term2
word_list_happiness1 <- predict(l_1$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
word_list_partner2 <- predict(l_2$model, c("partner//NOUN"), type = "nearest", top_n = 10)$`partner//NOUN`$term2
word_list_happiness2 <- predict(l_2$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2

partner1 <- subset(l_1$data, rownames(l_1$data) %in% word_list_partner1)
happiness1 <- subset(l_1$data, rownames(l_1$data) %in% word_list_happiness1)
partner2 <- subset(l_2$data, rownames(l_2$data) %in% word_list_partner2)
happiness2 <- subset(l_2$data, rownames(l_2$data) %in% word_list_happiness2)
```

```{r umap plot with genres, warning=FALSE, echo=FALSE}
options(ggrepel.max.overlaps = Inf) 
ggplot(partner1, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness1, aes(x=x, y=y, label=word, color='red')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness2, aes(x=x, y=y, label=word, color='magenta')) +
geom_text_repel() + theme_void() + geom_text_repel(data=partner2, aes(x=x, y=y, label=word, color='cyan')) + scale_color_manual(values=c("blue", "red", "magenta", 'cyan'),
                          labels = c("women - partner", "men - partner", "men - happiness", 'women - happiness'))
labs(title = "Most similar words to partner with word2vec - umap")
ggsave('../figs/word2vec_genres.png')
```
From this analysis we can see that male happiness is very closely associated with partnership/partner words, while female happiness is not. 

Let's now build a topic model.
```{r}
#  Overall topics
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(cleaned_data$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))

# have a look at the number of documents and terms in the matrix
dim(DTM)

# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]

# number of topics
K <- 20

# set random number generator seed
set.seed(9161)

# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))

terms(topicModel, 10)
```
```{r}
# Topics for Females
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(f_1$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))

# have a look at the number of documents and terms in the matrix
dim(DTM)

# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]

# number of topics
K <- 20

# set random number generator seed
set.seed(9161)

# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))

terms(topicModel, 10)
```
```{r}
# Topics for Males
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(m_2$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))

# have a look at the number of documents and terms in the matrix
dim(DTM)

# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]

# number of topics
K <- 20

# set random number generator seed
set.seed(9161)

# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))

terms(topicModel, 10)
```

```{r}
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
```

```{r}
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)

# format of the resulting object
attributes(tmResult)
```

```{r}
# visualize topics as word cloud
topicToViz <- 11 # change for your own topic of interest
topicToViz <- grep('husband', topicNames)[1] # Or select a topic by a term contained in its name

# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)

# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]

# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```

